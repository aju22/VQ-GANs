{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aju22/VQ-GANs/blob/main/VQ_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YW33X_BjkP84"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import os\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import utils as vutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VQ-GAN: Architecture\n",
        "![Architecture](https://miro.medium.com/v2/resize:fit:828/format:webp/1*JOrCybe84dKUvgiVNe0TaA.png)"
      ],
      "metadata": {
        "id": "PoN8shl_fKlD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HQS7mH6TNJmP"
      },
      "outputs": [],
      "source": [
        "class Swish(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VhHqdKCLkax8"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  \n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    \n",
        "    super().__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    \n",
        "    self.block = nn.Sequential(\n",
        "        nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True),\n",
        "        Swish(),\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
        "        nn.GroupNorm(num_groups=32, num_channels=out_channels, eps=1e-6, affine=True),\n",
        "        Swish(),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1)\n",
        "    )\n",
        "\n",
        "    if self.in_channels != self.out_channels:\n",
        "      self.channel_up = nn.Conv2d(in_channels, out_channels, 1, 1, 0)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "      if self.in_channels != self.out_channels:\n",
        "        return self.channel_up(x)+self.block(x)\n",
        "\n",
        "      else:\n",
        "        return x + self.block(x)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pF2QqyIHtDQV"
      },
      "outputs": [],
      "source": [
        "class UpSampleBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 1, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.interpolate(x, scale_factor=2.0)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0cDbmOfjyEHg"
      },
      "outputs": [],
      "source": [
        "class DownSampleBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, channels):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(channels, channels, 3, 2, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        pad = (0, 1, 0, 1)\n",
        "        x = F.pad(x, pad, mode=\"constant\", value=0)\n",
        "        return self.conv(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XBZKLjDbyFzI"
      },
      "outputs": [],
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.in_channels = channels\n",
        "\n",
        "        self.gn = nn.GroupNorm(32, channels)\n",
        "        self.q = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.k = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.v = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "        self.proj_out = nn.Conv2d(channels, channels, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        h_ = self.gn(x)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        b, c, h, w = q.shape\n",
        "\n",
        "        q = q.reshape(b, c, h*w)\n",
        "        q = q.permute(0, 2, 1)\n",
        "        k = k.reshape(b, c, h*w)\n",
        "        v = v.reshape(b, c, h*w)\n",
        "\n",
        "        attn = torch.bmm(q, k)\n",
        "        attn = attn * (int(c)**(-0.5))\n",
        "        attn = F.softmax(attn, dim=2)\n",
        "        attn = attn.permute(0, 2, 1)\n",
        "\n",
        "        A = torch.bmm(v, attn)\n",
        "        A = A.reshape(b, c, h, w)\n",
        "\n",
        "        return x + A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC15gvMM1hIq"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![]( https://miro.medium.com/max/828/0*PitQ20hZW7-HjPWr)"
      ],
      "metadata": {
        "id": "4jYEeVFEp2w2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "F2GI00Mq1ePr"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, image_channels, latent_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        channels = [128, 128, 128, 256, 256, 512]\n",
        "        attn_resolutions = [16]\n",
        "        num_res_blocks = 2\n",
        "        resolution = 256\n",
        "        \n",
        "        layers = [nn.Conv2d(image_channels, channels[0], 3, 1, 1)]\n",
        "        \n",
        "        for i in range(len(channels)-1):\n",
        "            in_channels = channels[i]\n",
        "            out_channels = channels[i + 1]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "                if resolution in attn_resolutions:\n",
        "                    layers.append(AttentionBlock(in_channels))\n",
        "            if i != len(channels)-2:\n",
        "                layers.append(DownSampleBlock(channels[i+1]))\n",
        "                resolution //= 2\n",
        "        \n",
        "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
        "        layers.append(AttentionBlock(channels[-1]))\n",
        "        layers.append(ResidualBlock(channels[-1], channels[-1]))\n",
        "        layers.append(nn.GroupNorm(32, channels[-1]))\n",
        "        layers.append(Swish())\n",
        "        layers.append(nn.Conv2d(channels[-1], latent_dim, 3, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n24iNDYe2NXw"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/828/0*Uv9K77hnuyYplsxw)"
      ],
      "metadata": {
        "id": "kpmbHJVIp8Zh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mJIP_twv2Lw6"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, image_channels, latent_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        channels = [512, 256, 256, 128, 128]\n",
        "        attn_resolutions = [16]\n",
        "        num_res_blocks = 3\n",
        "        resolution = 16\n",
        "\n",
        "        in_channels = channels[0]\n",
        "        layers = [nn.Conv2d(latent_dim, in_channels, 3, 1, 1),\n",
        "                  ResidualBlock(in_channels, in_channels),\n",
        "                  AttentionBlock(in_channels),\n",
        "                  ResidualBlock(in_channels, in_channels)]\n",
        "\n",
        "        for i in range(len(channels)):\n",
        "            out_channels = channels[i]\n",
        "            for j in range(num_res_blocks):\n",
        "                layers.append(ResidualBlock(in_channels, out_channels))\n",
        "                in_channels = out_channels\n",
        "                if resolution in attn_resolutions:\n",
        "                    layers.append(AttentionBlock(in_channels))\n",
        "            if i != 0:\n",
        "                layers.append(UpSampleBlock(in_channels))\n",
        "                resolution *= 2\n",
        "\n",
        "        layers.append(nn.GroupNorm(32, in_channels))\n",
        "        layers.append(Swish())\n",
        "        layers.append(nn.Conv2d(in_channels, image_channels, 3, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJWB_1mQp0iP"
      },
      "source": [
        "#CodeBook"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Note --> VQ-VAE Loss\n",
        "\n",
        "\n",
        "![](https://miro.medium.com/max/786/1*9kcpNwDdfVInvNSm_kM94A.webp)"
      ],
      "metadata": {
        "id": "S3woeekykpia"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WsiT5EOKpvm3"
      },
      "outputs": [],
      "source": [
        "class Codebook(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_codebook_vectors, latent_dim, beta):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.num_codebook_vectors = num_codebook_vectors\n",
        "        self.latent_dim = latent_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.num_codebook_vectors, self.latent_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.num_codebook_vectors, 1.0 / self.num_codebook_vectors)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.latent_dim)\n",
        "\n",
        "        d = torch.sum(z_flattened**2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - \\\n",
        "            2*(torch.matmul(z_flattened, self.embedding.weight.t()))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "\n",
        "        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2) #Stop gradient loss implementation\n",
        "\n",
        "        z_q = z + (z_q - z).detach() #This is preserving gradients in backprop\n",
        "\n",
        "        z_q = z_q.permute(0, 3, 1, 2)\n",
        "\n",
        "        return z_q, min_encoding_indices, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykopay--q1eE"
      },
      "source": [
        "#VQ-GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N-JrgQESqJXZ"
      },
      "outputs": [],
      "source": [
        "class VQGAN(nn.Module):\n",
        "    \n",
        "    def __init__(self, image_channels, latent_dim, num_codebook_vectors, beta, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(image_channels, latent_dim).to(device=device)\n",
        "        self.decoder = Decoder(image_channels, latent_dim).to(device=device)\n",
        "        self.codebook = Codebook(num_codebook_vectors, latent_dim, beta).to(device=device)\n",
        "        self.quant_conv = nn.Conv2d(latent_dim, latent_dim, 1).to(device=device)\n",
        "        self.post_quant_conv = nn.Conv2d(latent_dim, latent_dim, 1).to(device=device)\n",
        "\n",
        "    \n",
        "    #Encode from image space to latent(z) space\n",
        "    def encode(self, imgs):\n",
        "        \n",
        "        encoded_images = self.encoder(imgs)\n",
        "        quant_conv_encoded_images = self.quant_conv(encoded_images)\n",
        "        codebook_mapping, codebook_indices, q_loss = self.codebook(quant_conv_encoded_images)\n",
        "        return codebook_mapping, codebook_indices, q_loss\n",
        "\n",
        "    #Decode from latent(z) space to image space\n",
        "    def decode(self, z):\n",
        "        \n",
        "        post_quant_conv_mapping = self.post_quant_conv(z)\n",
        "        decoded_images = self.decoder(post_quant_conv_mapping)\n",
        "        return decoded_images\n",
        "    \n",
        "    def forward(self, imgs):\n",
        "        \n",
        "        codebook_mapping, codebook_indices, q_loss = self.encode(imgs)\n",
        "        decoded_images = self.decode(codebook_mapping)\n",
        "\n",
        "        return decoded_images, codebook_indices, q_loss\n",
        "\n",
        "   #Lambda is used by authors as a weighting factor betwen VQ-VAE Loss and GAN-Loss based on Perceptual Losses.\n",
        "    def calculate_lambda(self, perceptual_loss, gan_loss):\n",
        "        \n",
        "        last_layer = self.decoder.model[-1]\n",
        "        last_layer_weight = last_layer.weight\n",
        "        perceptual_loss_grads = torch.autograd.grad(perceptual_loss, last_layer_weight, retain_graph=True)[0]\n",
        "        gan_loss_grads = torch.autograd.grad(gan_loss, last_layer_weight, retain_graph=True)[0]\n",
        "\n",
        "        λ = torch.norm(perceptual_loss_grads) / (torch.norm(gan_loss_grads) + 1e-4)\n",
        "        λ = torch.clamp(λ, 0, 1e4).detach()\n",
        "        return 0.8 * λ\n",
        "\n",
        "    #Start the discriminator later, for generator to learn some basic reconstruction\n",
        "    @staticmethod\n",
        "    def adopt_weight(disc_factor, i, threshold, value=0.):\n",
        "        if i < threshold:\n",
        "            disc_factor = value\n",
        "        return disc_factor\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        self.load_state_dict(torch.load(path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLmf1FqwuUl-"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Taken from PatchGAN Paper\n"
      ],
      "metadata": {
        "id": "899j6zrboRYu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "OIAstB7juI-v"
      },
      "outputs": [],
      "source": [
        "class PatchDiscriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self, image_channels, num_filters_last=64, n_layers=3):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        layers = [nn.Conv2d(image_channels, num_filters_last, 4, 2, 1), nn.LeakyReLU(0.2)]\n",
        "        num_filters_mult = 1\n",
        "\n",
        "        for i in range(1, n_layers + 1):\n",
        "            num_filters_mult_last = num_filters_mult\n",
        "            num_filters_mult = min(2 ** i, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(num_filters_last * num_filters_mult_last, num_filters_last * num_filters_mult, 4,\n",
        "                          2 if i < n_layers else 1, 1, bias=False),\n",
        "                nn.BatchNorm2d(num_filters_last * num_filters_mult),\n",
        "                nn.LeakyReLU(0.2, True)\n",
        "            ]\n",
        "\n",
        "        layers.append(nn.Conv2d(num_filters_last * num_filters_mult, 1, 4, 1, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwTvSAFq_tr6"
      },
      "source": [
        "# VGG Perceptual Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PnTxPM-qwgz6"
      },
      "outputs": [],
      "source": [
        "class VGGPerceptualLoss(nn.Module):\n",
        "    \n",
        "    def __init__(self, resize=False):\n",
        "        \n",
        "        super().__init__()\n",
        "        blocks = []\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[:4].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[4:9].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[9:16].eval())\n",
        "        blocks.append(torchvision.models.vgg16(pretrained=True).features[16:23].eval())\n",
        "        \n",
        "        for bl in blocks:\n",
        "            for p in bl.parameters():\n",
        "                p.requires_grad = False\n",
        "        \n",
        "        self.blocks = nn.ModuleList(blocks)\n",
        "        self.transform = F.interpolate\n",
        "        self.resize = resize\n",
        "        self.register_buffer(\"mean\", torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
        "        self.register_buffer(\"std\", torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
        "\n",
        "    def forward(self, input, target, feature_layers=[0, 1, 2, 3]):\n",
        "        \n",
        "        input = (input-self.mean) / self.std\n",
        "        target = (target-self.mean) / self.std\n",
        "        \n",
        "        x = input\n",
        "        y = target\n",
        "        loss = 0.0\n",
        "        \n",
        "        for i, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            y = block(y)\n",
        "   \n",
        "            if i in feature_layers:\n",
        "            \n",
        "                loss += torch.abs(x - y).mean(dim=[1,2,3], keepdim=True)\n",
        "          \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx72a_Z3NQkk"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LSetJCPH5DF"
      },
      "outputs": [],
      "source": [
        "!wget https://www.robots.ox.ac.uk/~vgg/data/flowers/17/17flowers.tgz\n",
        "!tar -xvf \"17flowers.tgz\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Vel7o4teR4Rs"
      },
      "outputs": [],
      "source": [
        "class CustomDataSet(Dataset):\n",
        "    \n",
        "    def __init__(self, main_dir, size=None):\n",
        "        \n",
        "        self.main_dir = main_dir\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.CenterCrop(size),\n",
        "            transforms.Lambda(lambda image: ((image/127.5) - 1.0))\n",
        "            \n",
        "        ])\n",
        "        self.all_imgs = [fn for fn in os.listdir(main_dir) if fn.endswith(\"jpg\")]\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        return len(self.all_imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        img_loc = os.path.join(self.main_dir, self.all_imgs[idx])\n",
        "        image = Image.open(img_loc).convert(\"RGB\")\n",
        "        tensor_image = self.transform(image)\n",
        "        \n",
        "        return tensor_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zPy3TUOecUrM"
      },
      "outputs": [],
      "source": [
        "def trainer(path=None, batch_size=64):\n",
        "\n",
        "  train_data = CustomDataSet(path, size=32)\n",
        "  train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  return train_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ATFK4OdtkU"
      },
      "source": [
        "# Stage 1: Training VQ-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overall Objective Function:\n",
        "![](https://miro.medium.com/max/720/1*W4z0g6n8S0uvBv9bfcTsVA.webp)"
      ],
      "metadata": {
        "id": "FUQqZ4o0pEhd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/640/1*58MFhwgN3FaojP8KNahy7w.webp)"
      ],
      "metadata": {
        "id": "xjsFprcTpUL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/366/1*s8SWQzPbVn-mwBQ3do93gg.webp)"
      ],
      "metadata": {
        "id": "SYLPjTb9pUCw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "O__U3_o7c5Jc"
      },
      "outputs": [],
      "source": [
        "class TrainVQGAN:\n",
        "    def __init__(self,\n",
        "                 path = None, \n",
        "                 image_channels = 3, \n",
        "                 latent_dim = 256, \n",
        "                 num_codebook_vectors = 1024, \n",
        "                 beta = 0.25,\n",
        "                 lr = 3e-4,\n",
        "                 beta1 = 0.5,\n",
        "                 beta2 = 0.9,\n",
        "                 disc_factor = 1.,\n",
        "                 disc_start = 100,\n",
        "                 perceptual_loss_factor = 1.,\n",
        "                 rec_loss_factor = 1.,\n",
        "                 epochs = 10,\n",
        "                 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        \n",
        "        self.path = path\n",
        "        self.image_channels = image_channels \n",
        "        self.latent_dim = latent_dim \n",
        "        self.num_codebook_vectors = num_codebook_vectors \n",
        "        self.beta = beta\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.disc_factor = disc_factor\n",
        "        self.disc_start = disc_start\n",
        "        self.perceptual_loss_factor = perceptual_loss_factor\n",
        "        self.rec_loss_factor = rec_loss_factor\n",
        "        self.epochs = epochs\n",
        "        self.device = device\n",
        "        \n",
        "        self.vqgan = VQGAN(image_channels, latent_dim, num_codebook_vectors, beta, device).to(device=device)\n",
        "        self.discriminator = PatchDiscriminator(image_channels).to(device=device)\n",
        "        self.perceptual_loss = VGGPerceptualLoss().eval().to(device=device)\n",
        "        \n",
        "        self.opt_vq, self.opt_disc = self.configure_optimizers()\n",
        "        self.prepare_training()\n",
        "        #self.train(epochs, path, disc_factor, disc_start, perceptual_loss_factor, rec_loss_factor, device)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "  \n",
        "        opt_vq = torch.optim.Adam(\n",
        "            list(self.vqgan.encoder.parameters()) +\n",
        "            list(self.vqgan.decoder.parameters()) +\n",
        "            list(self.vqgan.codebook.parameters()) +\n",
        "            list(self.vqgan.quant_conv.parameters()) +\n",
        "            list(self.vqgan.post_quant_conv.parameters()),\n",
        "            lr=self.lr, eps=1e-08, betas=(self.beta1, self.beta2)\n",
        "        )\n",
        "        opt_disc = torch.optim.Adam(self.discriminator.parameters(),\n",
        "                                    lr=self.lr, eps=1e-08, betas=(self.beta1, self.beta2))\n",
        "\n",
        "        return opt_vq, opt_disc\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_training():\n",
        "        \n",
        "        os.makedirs(\"results\", exist_ok=True)\n",
        "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "\n",
        "    def train(self):\n",
        "        \n",
        "        train_dataset = trainer(path=self.path)\n",
        "        steps_per_epoch = len(train_dataset)\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            with tqdm(range(len(train_dataset))) as pbar:\n",
        "                \n",
        "                for i, imgs in zip(pbar, train_dataset):\n",
        "                    \n",
        "                    imgs = imgs.to(device=self.device)\n",
        "                    decoded_images, _, q_loss = self.vqgan(imgs)\n",
        "\n",
        "                    disc_real = self.discriminator(imgs)\n",
        "                    disc_fake = self.discriminator(decoded_images)\n",
        "\n",
        "                    disc_factor = self.vqgan.adopt_weight(self.disc_factor, epoch*steps_per_epoch+i, threshold=self.disc_start)\n",
        "\n",
        "                    perceptual_loss = self.perceptual_loss(imgs, decoded_images)\n",
        "                    rec_loss = torch.abs(imgs - decoded_images)\n",
        "                    perceptual_rec_loss = self.perceptual_loss_factor * perceptual_loss + self.rec_loss_factor * rec_loss\n",
        "                    perceptual_rec_loss = perceptual_rec_loss.mean()\n",
        "                    g_loss = -torch.mean(disc_fake)\n",
        "\n",
        "                    λ = self.vqgan.calculate_lambda(perceptual_rec_loss, g_loss)\n",
        "                    vq_loss = perceptual_rec_loss + q_loss + disc_factor * λ * g_loss\n",
        "\n",
        "                    d_loss_real = torch.mean(F.relu(1. - disc_real))\n",
        "                    d_loss_fake = torch.mean(F.relu(1. + disc_fake))\n",
        "                    gan_loss = disc_factor * 0.5*(d_loss_real + d_loss_fake)\n",
        "\n",
        "                    self.opt_vq.zero_grad()\n",
        "                    vq_loss.backward(retain_graph=True)\n",
        "\n",
        "                    self.opt_disc.zero_grad()\n",
        "                    gan_loss.backward()\n",
        "\n",
        "                    self.opt_vq.step()\n",
        "                    self.opt_disc.step()\n",
        "\n",
        "                    if i % 10 == 0:\n",
        "                        with torch.no_grad():\n",
        "                            real_fake_images = torch.cat((imgs[:4], decoded_images.add(1).mul(0.5)[:4]))\n",
        "                            vutils.save_image(real_fake_images, os.path.join(\"/content/results\", f\"{epoch}_{i}.jpg\"), nrow=4)\n",
        "\n",
        "                    pbar.set_postfix(\n",
        "                        VQ_Loss=np.round(vq_loss.cpu().detach().numpy().item(), 5),\n",
        "                        GAN_Loss=np.round(gan_loss.cpu().detach().numpy().item(), 3)\n",
        "                    )\n",
        "                    pbar.update(0)\n",
        "                \n",
        "                torch.save(self.vqgan.state_dict(), os.path.join(\"/content/checkpoints\", f\"vqgan_epoch_{epoch}.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "1b45c25cf01f419f97192f2a9695cc19",
            "dd2c98553cd247d9ba807053031b4fc6",
            "58a5995556ef4d9691a96edb101a703b",
            "91c3a63b24ca46d5939ac27221f6dfc4",
            "eefcadff006d43788f21ba4a3d770af4",
            "bf289e10d6fa43b8aa6a3326df03bea3",
            "e75ae5a34092489f9df16562edcbbc71",
            "b586547959a14c0d8d4e4ff7cbc6b4b8",
            "915cd734e9564937bcef126c904b2cc8",
            "9b6454a1cc8c4d13ba51cf10385c704c",
            "8b4c3dcc2ecb49f4a4454c37b087baf2"
          ]
        },
        "id": "YF7FR6cAE__f",
        "outputId": "b8a5d6cb-a104-4742-b49c-fe646e37eb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/528M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b45c25cf01f419f97192f2a9695cc19"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "vqgan_trainer = TrainVQGAN(path = '/content/jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiPhqlTBOEoF",
        "outputId": "ffbc0a39-a1eb-41ad-f361-bae704564d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [00:24<00:00,  1.12s/it, GAN_Loss=0, VQ_Loss=4.53]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.28it/s, GAN_Loss=0, VQ_Loss=1.8]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.29it/s, GAN_Loss=0, VQ_Loss=1.84]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.29it/s, GAN_Loss=0, VQ_Loss=1.24]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.28it/s, GAN_Loss=1.28, VQ_Loss=5.44]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.26it/s, GAN_Loss=0.991, VQ_Loss=-44.2]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.26it/s, GAN_Loss=0.984, VQ_Loss=0.173]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.26it/s, GAN_Loss=1.33, VQ_Loss=-1.24]\n",
            "100%|██████████| 22/22 [00:18<00:00,  1.22it/s, GAN_Loss=0.904, VQ_Loss=1.64]\n",
            "100%|██████████| 22/22 [00:17<00:00,  1.26it/s, GAN_Loss=1, VQ_Loss=-.084]\n"
          ]
        }
      ],
      "source": [
        "vqgan_trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLXvMHBFUxgU"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgKaglulSOBj",
        "outputId": "8d3c95d0-c48b-483c-e677-03da7a7b9faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'minGPT'...\n",
            "remote: Enumerating objects: 489, done.\u001b[K\n",
            "remote: Counting objects: 100% (489/489), done.\u001b[K\n",
            "remote: Compressing objects: 100% (230/230), done.\u001b[K\n",
            "remote: Total 489 (delta 266), reused 425 (delta 248), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (489/489), 1.43 MiB | 2.73 MiB/s, done.\n",
            "Resolving deltas: 100% (266/266), done.\n",
            "/content/minGPT\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/minGPT\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from minGPT==0.0.1) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->minGPT==0.0.1) (4.4.0)\n",
            "Installing collected packages: minGPT\n",
            "  Running setup.py develop for minGPT\n",
            "Successfully installed minGPT-0.0.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/minGPT.git\n",
        "%cd /content/minGPT\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4KpWRDRZU7mk"
      },
      "outputs": [],
      "source": [
        "from mingpt.model import GPT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://miro.medium.com/max/828/0*YhKKu0sDIEfe_vI3)"
      ],
      "metadata": {
        "id": "_7XiTpa5qH5Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-v9A9ONPVOR9"
      },
      "outputs": [],
      "source": [
        "class VQGANTransformer(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 num_codebook_vectors = 1024, \n",
        "                 pkeep = 0.5,\n",
        "                 device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.sos_token = 0\n",
        "        self.num_codebook_vectors = num_codebook_vectors\n",
        "        self.vqgan = self.load_vqgan(checkpoint_path = '/content/checkpoints',\n",
        "                                     device = device)\n",
        "\n",
        "        model_config = GPT.get_default_config()\n",
        "        model_config.vocab_size = num_codebook_vectors\n",
        "        model_config.block_size = 512\n",
        "        model_config.model_type = 'gpt2-medium'\n",
        "        \n",
        "        self.transformer = GPT(model_config)\n",
        "\n",
        "        self.pkeep = pkeep\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vqgan(\n",
        "                 image_channels = 3, \n",
        "                 latent_dim = 256, \n",
        "                 num_codebook_vectors = 1024, \n",
        "                 beta = 0.25,\n",
        "                 checkpoint_path=None,\n",
        "                 device=None\n",
        "                 ):\n",
        "        \n",
        "        model = VQGAN(image_channels, latent_dim, num_codebook_vectors, beta, device)\n",
        "        \n",
        "        if os.path.exists(checkpoint_path):\n",
        "          dirFiles = os.listdir(checkpoint_path)\n",
        "          dirFiles.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "          last_ckpt = os.path.join(checkpoint_path, dirFiles[-1])\n",
        "          \n",
        "          model.load_checkpoint(last_ckpt)\n",
        "        \n",
        "        model = model.eval()\n",
        "        return model\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_to_z(self, x):\n",
        "        quant_z, indices, _ = self.vqgan.encode(x)\n",
        "        indices = indices.view(quant_z.shape[0], -1)\n",
        "        return quant_z, indices\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def z_to_image(self, indices, p1=2, p2=2):\n",
        "      \n",
        "        ix_to_vectors = self.vqgan.codebook.embedding(indices).reshape(indices.shape[0], p1, p2, 256)\n",
        "        ix_to_vectors = ix_to_vectors.permute(0, 3, 1, 2)\n",
        "        image = self.vqgan.decode(ix_to_vectors)\n",
        "        return image\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, indices = self.encode_to_z(x)\n",
        "\n",
        "        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n",
        "        sos_tokens = sos_tokens.long().to(\"cuda\")\n",
        "\n",
        "        mask = torch.bernoulli(self.pkeep * torch.ones(indices.shape, device=indices.device))\n",
        "        mask = mask.round().to(dtype=torch.int64)\n",
        "        random_indices = torch.randint_like(indices, self.num_codebook_vectors)\n",
        "        new_indices = mask * indices + (1 - mask) * random_indices\n",
        "\n",
        "        new_indices = torch.cat((sos_tokens, new_indices), dim=1)\n",
        "\n",
        "        target = indices\n",
        "\n",
        "        logits, _ = self.transformer(new_indices[:, :-1])\n",
        "\n",
        "        return logits, target\n",
        "\n",
        "    def top_k_logits(self, logits, k):\n",
        "        v, ix = torch.topk(logits, k)\n",
        "        out = logits.clone()\n",
        "        out[out < v[..., [-1]]] = -float(\"inf\")\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self, x, c, steps, temperature=1.0, top_k=100):\n",
        "        self.transformer.eval()\n",
        "        x = torch.cat((c, x), dim=1)\n",
        "        \n",
        "        for k in range(steps):\n",
        "            logits, _ = self.transformer(x)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                logits = self.top_k_logits(logits, top_k)\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "        x = x[:, c.shape[1]:]\n",
        "        self.transformer.train()\n",
        "        return x\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, x):\n",
        "        log = dict()\n",
        "\n",
        "        _, indices = self.encode_to_z(x)\n",
        "        sos_tokens = torch.ones(x.shape[0], 1) * self.sos_token\n",
        "        sos_tokens = sos_tokens.long().to(\"cuda\")\n",
        "\n",
        "        start_indices = indices[:, :indices.shape[1] // 2]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1] - start_indices.shape[1])\n",
        "        half_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        start_indices = indices[:, :0]\n",
        "        sample_indices = self.sample(start_indices, sos_tokens, steps=indices.shape[1])\n",
        "        full_sample = self.z_to_image(sample_indices)\n",
        "\n",
        "        x_rec = self.z_to_image(indices)\n",
        "\n",
        "        log[\"input\"] = x\n",
        "        log[\"rec\"] = x_rec\n",
        "        log[\"half_sample\"] = half_sample\n",
        "        log[\"full_sample\"] = full_sample\n",
        "\n",
        "        return log, torch.concat((x, x_rec, half_sample, full_sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nvbIyrwWzI96"
      },
      "outputs": [],
      "source": [
        "def plot_images(images):\n",
        "    x = images[\"input\"]\n",
        "    reconstruction = images[\"rec\"]\n",
        "    half_sample = images[\"half_sample\"]\n",
        "    full_sample = images[\"full_sample\"]\n",
        "\n",
        "    fig, axarr = plt.subplots(1, 4)\n",
        "    axarr[0].imshow(x.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[1].imshow(reconstruction.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[2].imshow(half_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    axarr[3].imshow(full_sample.cpu().detach().numpy()[0].transpose(1, 2, 0))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EPCR_CTmihG"
      },
      "source": [
        "#Stge 2: Training Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uuKGUkAxZSdW"
      },
      "outputs": [],
      "source": [
        "class TrainTransformer():\n",
        "    def __init__(self, path = None, epochs = 10, device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        self.model = VQGANTransformer().to(device=device)\n",
        "        self.optim = self.configure_optimizers()\n",
        "        self.device = device\n",
        "        self.path = path\n",
        "        self.epochs = epochs\n",
        "        self.prepare_training()\n",
        "\n",
        "    @staticmethod\n",
        "    def prepare_training():\n",
        "        \n",
        "        os.makedirs(\"/content/transformer_results\", exist_ok=True)\n",
        "        os.makedirs(\"/content/transformer_checkpoints\", exist_ok=True)\n",
        "\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        decay, no_decay = set(), set()\n",
        "        whitelist_weight_modules = (nn.Linear, )\n",
        "        blacklist_weight_modules = (nn.LayerNorm, nn.Embedding)\n",
        "\n",
        "        for mn, m in self.model.transformer.named_modules():\n",
        "            for pn, p in m.named_parameters():\n",
        "                fpn = f\"{mn}.{pn}\" if mn else pn\n",
        "\n",
        "                if pn.endswith(\"bias\"):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, whitelist_weight_modules):\n",
        "                    decay.add(fpn)\n",
        "\n",
        "                elif pn.endswith(\"weight\") and isinstance(m, blacklist_weight_modules):\n",
        "                    no_decay.add(fpn)\n",
        "\n",
        "        param_dict = {pn: p for pn, p in self.model.transformer.named_parameters()}\n",
        "\n",
        "\n",
        "        optim_groups = [\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": 0.01},\n",
        "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=4.5e-06, betas=(0.9, 0.95))\n",
        "        return optimizer\n",
        "\n",
        "    def train(self):\n",
        "        \n",
        "        train_dataset = trainer(path = self.path)\n",
        "        \n",
        "        for epoch in range(self.epochs):\n",
        "            \n",
        "            with tqdm(range(len(train_dataset))) as pbar:\n",
        "                \n",
        "                for i, imgs in zip(pbar, train_dataset):\n",
        "                    self.optim.zero_grad()\n",
        "                    imgs = imgs.to(device=self.device)\n",
        "                    logits, targets = self.model(imgs)\n",
        "                    loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1))\n",
        "                    loss.backward()\n",
        "                    self.optim.step()\n",
        "                    pbar.set_postfix(Transformer_Loss=np.round(loss.cpu().detach().numpy().item(), 4))\n",
        "                    pbar.update(0)\n",
        "            \n",
        "            log, sampled_imgs = self.model.log_images(imgs[0][None])\n",
        "            \n",
        "            vutils.save_image(sampled_imgs, os.path.join(\"/content/transformer_results\", f\"transformer_{epoch}.jpg\"), nrow=4)\n",
        "            \n",
        "            torch.save(self.model.state_dict(), os.path.join(\"/content/transformer_checkpoints\", f\"transformer_{epoch}.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-OOczq-Ysms",
        "outputId": "b5361e84-4df3-414a-fab6-e128c0beb3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 303.88M\n"
          ]
        }
      ],
      "source": [
        "transformer_trainer = TrainTransformer(path = '/content/jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41TIHRh7au1T",
        "outputId": "6e968a6e-b67d-419d-fd0f-367167b9ed67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [00:17<00:00,  1.29it/s, Transformer_Loss=0.0093]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.50it/s, Transformer_Loss=0.0002]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.50it/s, Transformer_Loss=0.0002]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.50it/s, Transformer_Loss=0.0001]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.50it/s, Transformer_Loss=0.0001]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.49it/s, Transformer_Loss=0.0001]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.50it/s, Transformer_Loss=0.0001]\n",
            "100%|██████████| 22/22 [00:15<00:00,  1.43it/s, Transformer_Loss=0.0001]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.52it/s, Transformer_Loss=0]\n",
            "100%|██████████| 22/22 [00:14<00:00,  1.52it/s, Transformer_Loss=0]\n"
          ]
        }
      ],
      "source": [
        "transformer_trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMEQC1em0BMszFfRMispsRp",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b45c25cf01f419f97192f2a9695cc19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd2c98553cd247d9ba807053031b4fc6",
              "IPY_MODEL_58a5995556ef4d9691a96edb101a703b",
              "IPY_MODEL_91c3a63b24ca46d5939ac27221f6dfc4"
            ],
            "layout": "IPY_MODEL_eefcadff006d43788f21ba4a3d770af4"
          }
        },
        "dd2c98553cd247d9ba807053031b4fc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf289e10d6fa43b8aa6a3326df03bea3",
            "placeholder": "​",
            "style": "IPY_MODEL_e75ae5a34092489f9df16562edcbbc71",
            "value": "100%"
          }
        },
        "58a5995556ef4d9691a96edb101a703b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b586547959a14c0d8d4e4ff7cbc6b4b8",
            "max": 553433881,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_915cd734e9564937bcef126c904b2cc8",
            "value": 553433881
          }
        },
        "91c3a63b24ca46d5939ac27221f6dfc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b6454a1cc8c4d13ba51cf10385c704c",
            "placeholder": "​",
            "style": "IPY_MODEL_8b4c3dcc2ecb49f4a4454c37b087baf2",
            "value": " 528M/528M [00:02&lt;00:00, 241MB/s]"
          }
        },
        "eefcadff006d43788f21ba4a3d770af4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf289e10d6fa43b8aa6a3326df03bea3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75ae5a34092489f9df16562edcbbc71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b586547959a14c0d8d4e4ff7cbc6b4b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "915cd734e9564937bcef126c904b2cc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9b6454a1cc8c4d13ba51cf10385c704c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b4c3dcc2ecb49f4a4454c37b087baf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}